{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f850b36-1a6f-4d74-8bbd-4f2bc39b283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sc\n",
    "from sklearn import svm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTENC,ADASYN,SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4eeeb882-09a1-499e-8d55-ae8f9c97f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Polynomial Kernel:  61.852260198456456 %\n",
      "Accuracy with Gaussian Kernel:  63.43256155825065 %\n"
     ]
    }
   ],
   "source": [
    "#1. SVM with Polynomial Kernel\n",
    "\n",
    "X_train=sc.loadmat('/Users/ushakiranmayee/Documents/DataMining/Assignments/Assignment 4/Data for Assignment 4/Data for Problem 1/X_train.mat')\n",
    "Y_train=sc.loadmat('/Users/ushakiranmayee/Documents/DataMining/Assignments/Assignment 4/Data for Assignment 4/Data for Problem 1/Y_train.mat')\n",
    "X_test=sc.loadmat('/Users/ushakiranmayee/Documents/DataMining/Assignments/Assignment 4/Data for Assignment 4/Data for Problem 1/X_test.mat')\n",
    "Y_test=sc.loadmat('/Users/ushakiranmayee/Documents/DataMining/Assignments/Assignment 4/Data for Assignment 4/Data for Problem 1/Y_test.mat')\n",
    "\n",
    "xt=X_train['X_train']\n",
    "yt=Y_train['y_train']\n",
    "x_test=X_test['X_test']\n",
    "y_test=Y_test['y_test']\n",
    "\n",
    "\n",
    "def SVM_with_different_kernels(name,k):\n",
    "    total_ypred=[]\n",
    "    for cls in range(yt.shape[1]):\n",
    "        #for samples in range(xt.shape[0]):\n",
    "            model=svm.SVC(kernel=k,degree=2)\n",
    "            model.fit(xt,yt[:,cls])\n",
    "            y_predict_per_class=model.predict(x_test)\n",
    "            #print(y_predict_per_class)\n",
    "            total_ypred.append(y_predict_per_class)\n",
    "    total_ypred=np.array(total_ypred)\n",
    "    total_ypred=total_ypred.T\n",
    "    #print(total_ypred)\n",
    "    rows_with_zerost = np.sum(np.all((y_test == 0).all(axis=1)))\n",
    "    rows_with_zerosp = np.sum(np.all((total_ypred == 0).all(axis=1)))\n",
    "    #print(rows_with_zerost,rows_with_zerosp )\n",
    "    \n",
    "    \n",
    "    #Calculating Accuracy\n",
    "    \n",
    "    Accuracy=[]\n",
    "    for sample in range(y_test.shape[0]):\n",
    "        Common=0\n",
    "        Sum=0\n",
    "        for cls in range(y_test.shape[1]):\n",
    "            #print(sample)\n",
    "            if(total_ypred[sample][cls]==1 and y_test[sample][cls]==1):\n",
    "                Common+=1\n",
    "                Sum+=1\n",
    "            elif((total_ypred[sample][cls]==1 and y_test[sample][cls]==0) or (total_ypred[sample][cls]==0 and y_test[sample][cls]==1)):\n",
    "                Sum+=1\n",
    "        #print(Common,Sum,y_test[sample],total_ypred[sample])\n",
    "        if(Sum==0):\n",
    "            A=0\n",
    "        else:\n",
    "            A=Common/Sum\n",
    "        Accuracy.append(A)\n",
    "    Final_Accuracy=np.mean(Accuracy)\n",
    "    print(f\"Accuracy with {name} Kernel: \",Final_Accuracy*100,\"%\")\n",
    "    \n",
    "kernel={'Polynomial':'poly','Gaussian':'rbf'}\n",
    "for name,val in kernel.items():\n",
    "    SVM_with_different_kernels(name,val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "726eb52f-2ff3-4dcf-b6d8-3dee4db56391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE_mean with k=3 587.9039638275232\n",
      "SSE_mean with k=5 410.28892085150017\n",
      "SSE_mean with k=7 298.57799179993054\n"
     ]
    }
   ],
   "source": [
    "#2. K-means Clustering\n",
    "# Algorithm\n",
    "# 1. Pick random k clusters k=[3,5,7]\n",
    "# 2. Calculate the dst of each pt to these clusters\n",
    "# 3. Assign the nearest cluster to each data point\n",
    "# 4. Repeat the process for either 100 iterations or change in SSE for iterations is < 0.001 \n",
    "\n",
    "#1. Loading the Dataset\n",
    "Data=np.loadtxt('/Users/ushakiranmayee/Documents/DataMining/Assignments/Assignment 4/Data for Assignment 4/Data for Problem 2/seeds.txt')\n",
    "Data_list=Data.tolist()\n",
    "#2.1 k=[3,5,7]\n",
    "def K_Means_Clustering(k):\n",
    "    SSE_10=[0]*10\n",
    "    \n",
    "    for init in range(10):\n",
    "        #Initializing k random initial clusters\n",
    "        initial_clusters=random.sample(Data_list, k)\n",
    "        #print(initial_clusters)\n",
    "        #SSE=0\n",
    "        prev_SSE=1<<31\n",
    "        for iter in range(100):\n",
    "            #print(\"iter\",iter,init)\n",
    "            #e=[]\n",
    "            #initializing k lists to store the data in each cluster respectively\n",
    "            classification=[[] for j in range(k)] \n",
    "            for sample in Data:\n",
    "                #Calculating the euclidean distance between each sample and each cluster\n",
    "                e=[np.linalg.norm(sample-clu) for clu in initial_clusters]\n",
    "                #considering the index within e which has minimum distance value\n",
    "                index=np.argmin(e)\n",
    "                \n",
    "                classification[index].append(sample)\n",
    "    \n",
    "            #Got the samples distributed within each cluster. Now calculating the initial centroid of each cluster\n",
    "            mean_clusters=[np.mean(ele,axis=0) for ele in classification]\n",
    "            #For datapoint in each cluster calculating its Sq_distance from the mean to the datapoint\n",
    "            \n",
    "                \n",
    "            SSE=np.sum([np.sum((classification[cluster] - mean_clusters[cluster])**2) for cluster in range(k)])\n",
    "            if(prev_SSE-SSE)<0.001:\n",
    "                break\n",
    "            prev_SSE=SSE\n",
    "            initial_clusters=mean_clusters\n",
    "            \n",
    "        SSE_10[init]=SSE\n",
    "    \n",
    "    SSE_mean=np.mean(SSE_10)\n",
    "    print(f\"SSE_mean with k={k}\", SSE_mean)\n",
    "    \n",
    "k_values=[3,5,7]\n",
    "for samples in range(3):\n",
    "    K_Means_Clustering(k_values[samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62c3442d-90d6-4ff3-9a83-66a5bb6b8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Model - Random Forest\n",
    "\n",
    "def RandomForestClassification(method,X_train_encoded,X_test_encoded,y_resampled):\n",
    "    rf_model=RandomForestClassifier()\n",
    "    rf_model.fit(X_train_encoded, y_resampled)\n",
    "    predict_y_rf=rf_model.predict(X_test_encoded)\n",
    "    f1=f1_score(yt,predict_y_rf)\n",
    "    print(\"1. RandomForest\")\n",
    "    print(f\"F1-Score After the Balancing of the dataset by {method} is\",f1)\n",
    "\n",
    "#2. Model - DecisionTree\n",
    "\n",
    "def DecisionTreeClassification(method,X_train_encoded,X_test_encoded,y_resampled):\n",
    "    dt_model=DecisionTreeClassifier()\n",
    "    dt_model.fit(X_train_encoded, y_resampled)\n",
    "    predict_y_dt=dt_model.predict(X_test_encoded)\n",
    "    f1=f1_score(yt,predict_y_dt)\n",
    "    print(\"2. DecisionTree\")\n",
    "    print(f\"F1-Score After the Balancing of the dataset by {method} is\",f1)\n",
    "\n",
    "#3. ADABOOST Classifier\n",
    "\n",
    "def AdaboostClassification(method,X_train_encoded,X_test_encoded,y_resampled):\n",
    "    \n",
    "    adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "    adaboost_model.fit(X_train_encoded, y_resampled)\n",
    "    predict_y_ada=adaboost_model.predict(X_test_encoded)\n",
    "    f1=f1_score(yt,predict_y_ada)\n",
    "    print(\"3. ADABOOST Classifier\")\n",
    "    print(f\"F1-Score After the Balancing of the dataset by {method} is\",f1,end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de6c77b7-86cc-46a6-9bdf-a43a4048a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem_3\n",
    "#attributes=[' Status of existing checking account', 'Duration',' Credit history','Purpose','Credit amount','Savings account/bonds','Present employment since','Installment rate','Personal status','guarantors','Present residence since','Property','Age in years','Other installment plans','Housing','Number of existing credits at this bank','Job','Number of people being liable to provide maintenance for','Telephone','foreign worker']\n",
    "xdata_3=pd.read_csv(\"/Users/ushakiranmayee/Documents/DataMining/Assignments/Assignment 4/Data for Assignment 4/Data for Problem 3/German Credit Data.txt\",delimiter=\",\",header=None)\n",
    "\n",
    "y_3=xdata_3.iloc[:,-1]\n",
    "x_3=xdata_3.iloc[:,:-1]\n",
    "\n",
    "\n",
    "x,xt,y,yt=train_test_split(x_3,y_3,test_size=0.3,random_state=42,stratify=y_3)\n",
    "categorical_features=list(x_3.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f68d4fa5-36d1-450a-8145-8b3c0e8f11f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method-SMOTENC\n",
      "\n",
      "1. RandomForest\n",
      "F1-Score After the Balancing of the dataset by SMOTENC is 0.8341013824884792\n",
      "2. DecisionTree\n",
      "F1-Score After the Balancing of the dataset by SMOTENC is 0.74\n",
      "3. ADABOOST Classifier\n",
      "F1-Score After the Balancing of the dataset by SMOTENC is 0.801955990220049\n",
      "\n",
      "\n",
      "Method-ADASYN\n",
      "\n",
      "1. RandomForest\n",
      "F1-Score After the Balancing of the dataset by ADASYN is 0.8296943231441049\n",
      "2. DecisionTree\n",
      "F1-Score After the Balancing of the dataset by ADASYN is 0.7167919799498748\n",
      "3. ADABOOST Classifier\n",
      "F1-Score After the Balancing of the dataset by ADASYN is 0.8094117647058824\n",
      "\n",
      "\n",
      "Method-SMOTE\n",
      "\n",
      "1. RandomForest\n",
      "F1-Score After the Balancing of the dataset by SMOTE is 0.8259860788863109\n",
      "2. DecisionTree\n",
      "F1-Score After the Balancing of the dataset by SMOTE is 0.7707317073170733\n",
      "3. ADABOOST Classifier\n",
      "F1-Score After the Balancing of the dataset by SMOTE is 0.8229665071770335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.1 Using SMOTENC model for handling the class Imbalance Problem with OneHotEncoder\n",
    "\n",
    "smote_nc=SMOTENC(categorical_features=categorical_features, random_state=42)\n",
    "x_resampled, y_resampled=smote_nc.fit_resample(x,y)\n",
    "\n",
    "\n",
    "#Converting the Categorical Data to Numerical Data for training the models\n",
    "encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_encoded = encoder.fit_transform(x_resampled)\n",
    "X_test_encoded=encoder.fit_transform(xt)\n",
    "\n",
    "print(\"Method-SMOTENC\",end=\"\\n\\n\")\n",
    "\n",
    "RandomForestClassification(\"SMOTENC\",X_train_encoded,X_test_encoded,y_resampled)\n",
    "DecisionTreeClassification(\"SMOTENC\",X_train_encoded,X_test_encoded,y_resampled)\n",
    "AdaboostClassification(\"SMOTENC\",X_train_encoded,X_test_encoded,y_resampled)\n",
    "\n",
    "#3.2 Using ADASYN model for handling the class Imbalance Problem with OneHotEncoder\n",
    "\n",
    "x_en=encoder.fit_transform(x)\n",
    "xt_en=encoder.fit_transform(xt)\n",
    "#2. Adaptive Synthetic Sampling\n",
    "\n",
    "ADA_SYN_MODEL=ADASYN()\n",
    "x_resampled1,y_resampled1=ADA_SYN_MODEL.fit_resample(x_en,y)\n",
    "\n",
    "print(\"\\nMethod-ADASYN\",end=\"\\n\\n\")\n",
    "\n",
    "RandomForestClassification(\"ADASYN\",x_resampled1,xt_en,y_resampled1)\n",
    "DecisionTreeClassification(\"ADASYN\",x_resampled1,xt_en,y_resampled1)\n",
    "AdaboostClassification(\"ADASYN\",x_resampled1,xt_en,y_resampled1)\n",
    "\n",
    "\n",
    "#3.3 Using SMOTE model for handling the class Imbalance Problem with Label Encoder\n",
    "label_encoder=LabelEncoder()\n",
    "for col in categorical_features:\n",
    "    x[col]=label_encoder.fit_transform(x[col])\n",
    "\n",
    "for col in categorical_features:\n",
    "    xt[col]=label_encoder.fit_transform(xt[col])\n",
    "\n",
    "#print(x)\n",
    "smote=SMOTE()\n",
    "x_rs3,y_rs3=smote.fit_resample(x,y)\n",
    "\n",
    "print(\"\\nMethod-SMOTE\",end=\"\\n\\n\")\n",
    "\n",
    "RandomForestClassification(\"SMOTE\",x_rs3,xt,y_rs3)\n",
    "DecisionTreeClassification(\"SMOTE\",x_rs3,xt,y_rs3)\n",
    "AdaboostClassification(\"SMOTE\",x_rs3,xt,y_rs3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
